
\section{Conjugate Gradient}
\sectionauthor{Benjamin Knöbel del Olmo, Leo Bergmann, (Chuyang Wang)}

Um lineare Gleichungssysteme (LGS) effizient und exakt zu lösen, wird Conjugate Gradient verwendet, welcher auch auf der Minimabestimmung basiert.


Die Formel $Ax = b$ lässt sich auf ein LGS anwenden, wobei A eine $ n \times n $-Matrix (invariant unter Transposition) und $ b,x $ Vektoren sind. Eine quadratische Funktion $ f(x) = \frac{1}{2}xAx-bx+c $ wird definiert, (mit selben A,b und x), dessen komponentenweise Ableitung genau dann null ist, wenn das ursprüngliche LGS nach x gelöst ist, da gilt: $f'(x) = Ax-b = 0$. Daher ist das Optimieren der dem LGS entsprechenden Funktion äquivalent zum Lösen ebenjenes.


Der Algorithmus funktioniert folgendermaßen: Conjugate Gradient läuft das Residuum (der dem Gradienten entgegengesetzte Vektor, $ r = b-Ax $) ab, aber korrigiert die „Abstiegsrichtung“ unter Betrachtung aller vorher gegangenen Residuen so, dass die Abstiegsrichtung A-orthogonal (engl. conjugate, def.: $x^{T}Ay = 0$, x,y A-orthogonal) zu allen vorherigen Residuen ist. Daraus folgt, dass wir nach maximal $n$-Schritten am Ziel ankommen, da wir dann alle $n$ Dimensionen der $ n \times n $-Matrix optimal abgegangen sind. Der Algorithmus ist durch zwei Abbruchbedingungen definiert, zum einen, wenn $n$ Iterationen erreicht sind, zum andern wenn die Differenz zum Endergebnis, also dasss $ f'(x) = 0 $ gilt, klein genug ist.

Gradient Descent und Conjugate Gradient ermöglicht es uns also, lineare Gleichungssysteme zu lösen und dadurch Daten, die in Matrizen gespeichert sind, weiterzuverarbeiten. Ein Beispiel wäre die Lösung der Gleichung $ D^{-1} m = j $, auf die im Kapitel \enquote{Wiener Filter} \cref{k4.2.wiener.filter} eingegangen wird.
